###  神经网络架构搜索（NAS）笔记

- **背景问题**：
    - 传统压缩技术（剪枝、量化）无法从根本上设计高效的小模型。
    - 手工设计网络架构难以同时满足多目标需求，如：
        - **延迟低**：适应实时应用。
        - **存储少**：适配移动设备。
        - **能耗低**：节能设计。
        - **高精度**：维持模型预测性能。
- **NAS概念**：
    - 把神经网络的结构和参数量作为**搜索空间**。
    - 使用优化算法，结合性能评估，找到合适架构。
- **目标**：
    - 自动化生成高效神经网络架构，满足延迟、存储、能耗与精度多方面需求。

#### **2. NAS的基本概念**

1. **基础网络模块**：
    
    - 包括线性变换、卷积、分组卷积等。
    - 现代网络如ResNet：
        - **残差模块**：减少梯度消失，提升训练效率。
        - 结构简单，支持堆叠扩展。
    - **Transformer多头注意力模块**：
        - 并行处理能力强。
        - 在NLP和CV任务中表现优异。
2. **模型搜索目标**：
    
    - 需要设计模型在**精度-效率**间找到平衡点。
    - 传统手工设计网络难以高效满足多个目标。

#### **3. 搜索空间**

- **定义**：
    
    - 搜索空间是所有可能的网络架构组合集合。
    - 表现形式：
        - 单层神经元的参数组合。
        - 不同网络模块的嵌套组合。
- **搜索空间的划分**：
    
    1. **单元级搜索空间**：
        - 针对每一层的操作（如卷积、池化等）。
        - 示例公式：假设网络有 MMM 种基础模型、NNN 种合并方式、LLL 层，搜索空间大小为： (2⋅M⋅N)L(2 \cdot M \cdot N)^L(2⋅M⋅N)L
        - 例：有5种基础模型、2种组合方式、5层网络，搜索空间为 3.2×10113.2 \times 10^{11}3.2×1011。
    2. **网络级搜索空间**：
        - 针对整个网络结构进行优化：
            - 残差模块深度。
            - 输入输出维度调整。
            - 图像解析度等。
        - 搜索难度高，但优化空间大。
- **搜索空间与硬件限制**：
    
    - 实际硬件（如GPU、手机）限制：
        - 存储容量有限。
        - 计算速度不同。
    - 搜索空间需要根据硬件特性调整，避免无效计算。

#### **4. 搜索策略**

1. **网格搜索（Grid Search）**：
    
    - 列举所有可能架构组合并进行逐一测试。
    - 优点：简单易实现。
    - 缺点：计算量大，不适用于高维空间。
2. **随机搜索（Random Search）**：
    
    - 在搜索空间中随机采样部分架构进行测试。
    - 比网格搜索更高效，但不能保证全局最优。
3. **强化学习（Reinforcement Learning）**：
    
    - 利用生成器（如RNN变体）生成网络架构。
    - 训练后将性能反馈给生成器，逐步优化。
    - 示例：
        - 使用RNN生成候选架构。
        - 在数据集上评估模型精度，反馈给生成器更新策略。
4. **梯度下降（Gradient Descent）**：
    
    - 将架构选择转化为概率优化问题：
        - 每层的操作赋予概率。
        - 优化目标函数，调整概率分布。
    - 优势：能高效优化连续空间。
5. **进化算法（Evolutionary Search）**：
    
    - 模拟生物进化：
        - 子网络的“选择-变异-交叉”。
        - 通过性能评估保留优质网络。
    - 适合非连续搜索空间。

#### **5. 模型性能评估**

1. **传统评估方法**：
    - 每次测试需训练完整模型，耗时耗资源。
2. **低成本评估方法**：
    1. **权重继承（Inherit Weight）**：
        - 新模型的参数从旧模型映射而来。
        - 减少重复训练成本。
    2. **超网络（Hypernetwork）**：
        - 将网络架构视为生成网络的输出：
            - 随机采样架构。
            - 生成网络参数并优化损失函数。
        - 避免独立训练每个架构。

#### **6. 硬件主导的NAS**

1. **传统问题**：
    - 传统NAS针对通用GPU优化，忽略移动设备特性。
    - 移动设备中隐藏层维度增加会显著提升延迟。
2. **ProxylessNAS**：
    - 构建大规模过参数模型。
    - 单次训练中对架构剪枝，保留有效路径。
    - 内存复杂度从 O(N)O(N)O(N) 降至 O(1)O(1)O(1)。

#### **7. Once-For-All（OFA）模型**

- **核心理念**：
    - 设计适配多设备的通用网络。
    - **过程**：
        - 在大模型中采样子网络。
        - 针对不同设备需求优化架构。
- **应用场景**：
    1. **NLP**：
        - 针对不同设备搜索Transformer架构。
        - 显著降低模型延迟。
    2. **GAN生成**：
        - 提高生成模型在不同硬件上的通用性。
    3. **姿态检测**：
        - 搜索轻量化网络，提升实时检测性能。
