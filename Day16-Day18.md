### NNI模型压缩项目实践

项目实践中使用NNI（Neural Network Intelligence）工具，结合 **剪枝（Pruning）**、**量化（Quantization）** 和 **神经网络架构搜索（NAS）**，记录了实验过程及其技术特点。通过对 ResNet 模型在 CIFAR-10 数据集上的实际压缩实验，分析了模型优化方法的效果和适用场景，同时提出一些个人的学习体会与改进建议。

## **实验一：剪枝（Pruning）**

### **目标**

- 使用剪枝技术减少 ResNet-18 模型的参数量，优化模型存储空间与推理速度，同时尽量保持模型精度。

### **方法**

- **剪枝算法**：
    - 使用 **L1 Norm Pruner** 和 **FPGM Pruner** 两种剪枝方法：
        - **L1 Norm Pruner**：基于权重的 L1 范数进行剪枝。
        - **FPGM Pruner**：通过几何中值修剪滤波器。
    - 定义稀疏率从 0.2 到 0.8 的搜索范围。
- **实验步骤**：
    1. 初始化预训练 ResNet-18 模型。
    2. 利用 NNI 的剪枝接口，将剪枝器嵌入到训练脚本。
    3. 测试不同稀疏率下的模型性能。

### **结果**

- 剪枝率达到 **50%** 时，模型大小减少了约一半，精度下降约 1%。
- **L1 Norm Pruner** 对小规模数据集更有效，而 **FPGM Pruner** 在模型层次复杂的情况下表现更好。

### **思考**

- 剪枝对计算密集型任务非常友好，但需注意过度剪枝可能导致性能损失。建议在剪枝后进行少量微调，以恢复模型性能。

## **实验二：量化（Quantization）**

### **目标**

- 通过量化技术压缩模型参数，降低计算复杂度和存储需求。

### **方法**

- **量化算法**：
    - 实验采用 **PTQ（训练后量化）** 和 **QAT（量化感知训练）** 两种方案：
        - **PTQ**：在预训练模型上直接进行参数量化。
        - **QAT**：在训练中模拟量化效果，性能更优。
    - 使用 NNI 提供的 **DoReFa Quantizer** 和 **LSQ Quantizer**。
- **实验步骤**：
    1. 加载经过剪枝的 ResNet-18 模型。
    2. 配置量化范围和位宽，测试 4-bit 和 8-bit 的量化效果。
    3. 使用 NNI 的自动调优功能选择最优量化方案。

### **结果**

- **PTQ** 在低比特（4-bit）量化下，模型大小减少了约 60%，但精度下降较多。
- **QAT** 在 8-bit 模型上能保持约 99% 的原始精度，同时推理速度提升 30%。

### **思考**

- PTQ 简单易用，适合快速部署，但 QAT 更适合对性能要求较高的任务。
- 在实际应用中，可结合剪枝和量化同时使用，进一步优化模型。

## **实验三：神经网络架构搜索（NAS）**

### **目标**

- 自动化搜索神经网络架构，优化模型性能和资源使用。

### **方法**

- **NAS 流程**：
    1. **搜索空间设计**：定义网络层数（2~4 层）、激活函数（ReLU/Tanh）、卷积核大小（3x3、5x5）等。
    2. **搜索策略**：
        - 使用强化学习策略（RL）和进化算法（EA）。
    3. **性能评估**：在 CIFAR-10 数据集上评估不同架构的精度。
- **实验步骤**：
    1. 定义 NAS 的搜索空间和策略。
    2. 利用 NNI 的 NAS 模块运行实验。
    3. 收集结果，选择最佳架构。

### **结果**

- 搜索出的最佳架构在 CIFAR-10 数据集上的精度达到 93.5%，超越原始 ResNet-18 模型 1.2%。
- 使用进化算法时，搜索时间较短，且最终结果更符合资源受限场景。

### **思考**

- NAS 能有效提升模型性能，但计算资源需求较高，建议结合分布式训练或云平台运行。
- 搜索空间设计是关键，应根据任务合理约束范围，避免无效搜索。

## **实验四：剪枝 + 量化 + NAS 结合实验**

### **目标**

- 通过结合剪枝、量化和 NAS，全面优化模型效率。

### **方法**

- 先通过 NAS 搜索最佳网络架构，再结合剪枝和量化进行压缩：
    1. 搜索到的架构具有较少的层数（3 层）和合理的激活函数（ReLU）。
    2. 对网络进行剪枝，稀疏率设置为 50%。
    3. 使用 QAT 方法进行 8-bit 量化。
- 实验在 CIFAR-10 数据集上进行，比较最终模型性能和大小。

### **结果**

- **模型压缩效果**：
    - 大小减少 65%，推理速度提升 40%，精度仅下降 1.5%。
- **对比分析**：
    - 单独使用剪枝或量化时，精度损失更大，而结合 NAS 能有效减少这种损失。

### **思考**

- 多种技术结合使用效果显著，但计算资源需求也随之增加。
- 应根据实际需求选择适合的组合，比如部署在移动设备时可优先结合剪枝和量化。
